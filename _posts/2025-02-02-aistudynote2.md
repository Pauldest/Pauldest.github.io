---
layout: post
title: 2_AI学习笔记 NLP模型总览
date: 2025-02-02 19:20 +0800
description:
image:
category: AI学习笔记
tags:
---

自然语言处理（NLP）模型经历了从基于规则到统计方法，再到深度学习的演变。以下是各阶段的经典模型及其核心贡献：

---

#### **一、传统统计模型**
1. **n-gram 模型**  
   - **原理**：基于马尔可夫假设，用前 \(n-1\) 个词预测当前词的概率（如二元模型 \(P(w_i | w_{i-1})\)）。  
   - **应用**：早期语言模型、文本生成、拼写纠错。  
   - **局限**：无法建模长距离依赖，数据稀疏问题严重。

2. **TF-IDF（词频-逆文档频率）**  
   - **原理**：通过词频和逆文档频率加权，衡量词语在文档中的重要性。  
   - **应用**：信息检索、文本分类（如垃圾邮件过滤）。  
   - **公式**：\( \text{TF-IDF}(t, d) = \text{TF}(t, d) \times \log\left(\frac{N}{\text{DF}(t)}\right) \)。

---

#### **二、词嵌入模型**
3. **Word2Vec（2013）**  
   - **原理**：通过浅层神经网络学习词向量（CBOW 和 Skip-Gram 架构）。  
   - **特点**：  
     - 语义相似性：向量空间中“国王 - 男人 + 女人 ≈ 女王”。  
     - 应用：词类比、文本相似度计算。  
   - **局限**：静态词向量，无法处理多义词。

4. **GloVe（2014）**  
   - **原理**：结合全局词共现矩阵与局部上下文窗口，优化词向量。  
   - **公式**：\( J = \sum_{i,j} f(X_{ij}) (w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2 \)。  
   - **优势**：更好捕捉全局统计信息。

---

#### **三、循环神经网络（RNN）**
5. **RNN 与 LSTM（1997）**  
   - **RNN**：通过循环结构处理序列数据，但存在梯度消失/爆炸问题。  
   - **LSTM**：引入门控机制（输入门、遗忘门、输出门），缓解长序列依赖问题。  
   - **应用**：机器翻译、文本生成（如生成诗歌）。  
   - **代表模型**：Seq2Seq（2014）用于机器翻译（编码器-解码器架构）。

---

#### **四、注意力机制与 Transformer**
6. **Transformer（2017）**  
   - **核心创新**：  
     - **自注意力机制**：计算序列中所有位置的关联权重。  
     - **多头注意力**：并行学习不同子空间的语义关系。  
     - **位置编码**：替代 RNN 的位置信息建模。  
   - **公式**（自注意力）：  
     \[
     \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
     \]
   - **影响**：成为现代 NLP 的基础架构，取代 RNN。

---

#### **五、预训练语言模型**
7. **BERT（2018）**  
   - **架构**：基于 Transformer 编码器的双向模型。  
   - **预训练任务**：  
     - **Masked Language Model (MLM)**：随机遮盖词语并预测。  
     - **Next Sentence Prediction (NSP)**：判断句子对是否连续。  
   - **特点**：支持下游任务微调，在 GLUE 基准上刷新记录。

8. **GPT 系列（2018-2020）**  
   - **架构**：基于 Transformer 解码器的单向模型。  
   - **训练方式**：自回归语言建模（预测下一个词）。  
   - **演进**：  
     - GPT-1：1.17 亿参数。  
     - GPT-3：1750 亿参数，支持零样本/少样本学习。  
   - **应用**：文本生成、对话系统（如 ChatGPT）。

---

#### **六、其他重要模型**
9. **ELMo（2018）**  
   - **特点**：动态词向量，结合双向 LSTM 的上下文信息。  
   - **公式**：\( \text{ELMo}(w) = \gamma \sum_{k} s_k \cdot h_{k}^{LM}(w) \)。  
   - **局限**：计算复杂度高，后被 Transformer 取代。

10. **T5（2020）**  
    - **思想**：“Text-to-Text”统一框架，将所有任务转换为文本生成形式。  
    - **示例**：翻译任务输入“translate English to German: Hello”，输出“Hallo”。

---

### **模型演进总结**
| **阶段**     | **代表模型**    | **核心突破**                  |
| ------------ | --------------- | ----------------------------- |
| 统计方法     | n-gram, TF-IDF  | 基于概率和词频的文本表示      |
| 词嵌入       | Word2Vec, GloVe | 分布式语义表示                |
| 循环神经网络 | LSTM, Seq2Seq   | 序列建模与长距离依赖处理      |
| 注意力机制   | Transformer     | 并行化全局上下文建模          |
| 预训练模型   | BERT, GPT       | 大规模无监督预训练 + 微调范式 |

---

### **应用场景对比**
- **文本分类**：BERT 微调（如情感分析）。  
- **生成任务**：GPT-3（如故事创作）。  
- **序列标注**：BiLSTM + CRF（如命名实体识别）。  
- **机器翻译**：Transformer 或 T5。

---

### **未来方向**
1. **高效模型**：模型压缩（如 DistilBERT）、稀疏注意力。  
2. **多模态融合**：CLIP（图文结合）、Florence（视频理解）。  
3. **可控生成**：通过提示工程（Prompt Engineering）引导模型输出。
